{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DhZ_bYwXQjLp"
   },
   "source": [
    "#102 Flowers Classification\n",
    "This Project is done using Google Colaboratory.\n",
    "1st you have to mount google drive with Colab and place dataset in zipped format on Google drive to access it.\n",
    "This dataset contains flower images of 102 categories. Training images are 6552, validation images are 818 and test images are 819.\n",
    "I have achieved top1 accuracy 95.73% and top5 accuracy 99.63% on test set. This project is done using a ResNet18(pretrained on imagenet). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vRbcUB6RR0OJ"
   },
   "source": [
    "Next cell is for installation of PyTorch on Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "9Z7eez-xNewr",
    "outputId": "57175881-4906-46de-b5f4-e7e9ecbe5974"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-12-20 06:27:29--  https://s3.amazonaws.com/content.udacity-data.com/courses/nd188/flower_data.zip\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.81.83\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.81.83|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 311442766 (297M) [application/zip]\n",
      "Saving to: ‘flower_data.zip’\n",
      "\n",
      "flower_data.zip     100%[===================>] 297.01M  16.7MB/s    in 20s     \n",
      "\n",
      "2018-12-20 06:27:50 (14.9 MB/s) - ‘flower_data.zip’ saved [311442766/311442766]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://s3.amazonaws.com/content.udacity-data.com/courses/nd188/flower_data.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MpcUkczwTNYK"
   },
   "outputs": [],
   "source": [
    "!cp -r \"./flower_data/valid\" \"./flower_data/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "fHjl9ZbdNWVL",
    "outputId": "a195c156-f9f5-470b-9d1a-85ec2f0099c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3FDUy0ZroA9s",
    "outputId": "d7fb5adc-c1c6-46f5-dae4-7b3b1ac76c0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (5.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q http://download.pytorch.org/whl/cu91/torch-0.4.0-cp36-cp36m-linux_x86_64.whl\n",
    "!pip3 install torchvision\n",
    "!pip3 install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PkALZSsrp3A9"
   },
   "outputs": [],
   "source": [
    "!unzip \"drive/My Drive/flower_data.zip\" # this line copies the flowers dataset from Google drive to Google Colab and also Unzip it for further processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mQkox1Quo_fd"
   },
   "outputs": [],
   "source": [
    "# Imports here\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torch.utils.data as data\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import json\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R6Gzo5buSHmd"
   },
   "source": [
    "#Load Data\n",
    "We will use torchvision and torch.utils.data packages for loading the data.\n",
    "For the training, i have applied transformations such as random scaling, cropping, and flipping. This will help the network generalize leading to better performance. I also made it sure that the input data is resized to 224x224 pixels as required by the pre-trained networks.\n",
    "\n",
    "The validation and testing sets are used to measure the model's performance on data it hasn't seen yet. For this i have not performed any scaling or rotation transformations, but i had resized and then cropped the images to the appropriate size.\n",
    "\n",
    "The pre-trained networki have used was trained on the ImageNet dataset where each color channel was normalized separately. For all three sets i have normalized the means and standard deviations of the images to what the network expects. For the means, it's [0.485, 0.456, 0.406] and for the standard deviations [0.229, 0.224, 0.225], calculated from the ImageNet images. These values will shift each color channel to be centered at 0 and range from -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sqt0QG_LqhNn"
   },
   "outputs": [],
   "source": [
    "data_dir = './flower_data'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/valid'\n",
    "test_dir = data_dir + '/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BFIePvAHqq5k"
   },
   "outputs": [],
   "source": [
    "# TODO: Define your transforms for the training, validation, and testing sets\n",
    "# defining data transforms for training, validation and test data and also normalizing whole data\n",
    "data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomRotation(45),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "         ]),\n",
    "        'valid': transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "         ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "         ])\n",
    "    }\n",
    "\n",
    "# TODO: Load the datasets with ImageFolder\n",
    "# loading datasets with PyTorch ImageFolder\n",
    "image_datasets = {\n",
    "        x: datasets.ImageFolder(root=data_dir + '/' + x, transform=data_transforms[x])\n",
    "        for x in list(data_transforms.keys())\n",
    "    }\n",
    "\n",
    "# TODO: Using the image datasets and the trainforms, define the dataloaders\n",
    "# defining data loaders to load data using image_datasets and transforms, here we also specify batch size for the mini batch\n",
    "dataloaders = {\n",
    "        x: data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=2)\n",
    "        for x in list(image_datasets.keys())\n",
    "    }\n",
    "dataset_sizes = {\n",
    "        x: len(dataloaders[x].dataset) \n",
    "        for x in list(image_datasets.keys())\n",
    "    } \n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Uv1_OgR3q9ft",
    "outputId": "9dc96f18-cbee-4ec0-ed16-701c1b523acc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': 818, 'train': 6552, 'valid': 818}"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sizes # printing dataset's sizes for training, validation and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EVAiu-n7TxC9"
   },
   "source": [
    "#Label mapping\n",
    "I had load in a mapping from category label to category name. I got this in the file cat_to_name.json. It's a JSON object which i have read in with the json module. This gave a dictionary mapping the integer encoded categories to the actual names of the flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "colab_type": "code",
    "id": "m0HfGOasWYUw",
    "outputId": "61a41008-ab7d-494e-a8e4-c580f2a11fec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-12-20 06:42:57--  https://raw.githubusercontent.com/udacity/pytorch_challenge/master/cat_to_name.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2218 (2.2K) [text/plain]\n",
      "Saving to: ‘cat_to_name.json.1’\n",
      "\n",
      "\r",
      "cat_to_name.json.1    0%[                    ]       0  --.-KB/s               \r",
      "cat_to_name.json.1  100%[===================>]   2.17K  --.-KB/s    in 0s      \n",
      "\n",
      "2018-12-20 06:42:57 (43.8 MB/s) - ‘cat_to_name.json.1’ saved [2218/2218]\n",
      "\n",
      "--2018-12-20 06:42:57--  http://flower_data/\n",
      "Resolving flower_data (flower_data)... failed: Name or service not known.\n",
      "wget: unable to resolve host address ‘flower_data’\n",
      "FINISHED --2018-12-20 06:42:57--\n",
      "Total wall clock time: 0.2s\n",
      "Downloaded: 1 files, 2.2K in 0s (43.8 MB/s)\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/udacity/pytorch_challenge/master/cat_to_name.json \"flower_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "az9w508arjGF"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('./cat_to_name.json', 'r') as f:\n",
    "    cat_to_name = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ieCbe8kc0HZ_"
   },
   "outputs": [],
   "source": [
    "# changing categories to their actual names \n",
    "for i in range(0,len(class_names)):\n",
    "    class_names[i] = cat_to_name.get(class_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aiPZBmFgUR9h"
   },
   "source": [
    "#Visualize a few images\n",
    "Let's visualize a few training images so as to understand the data augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 758
    },
    "colab_type": "code",
    "id": "4VdYtwG8rsLc",
    "outputId": "ef4b6e75-8f78-4af7-dd9e-82fe7e816bb0"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-ca4e29cc8756>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Get a batch of training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Make a grid from batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mbase_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    228\u001b[0m                 self.worker_manager_thread = threading.Thread(\n\u001b[1;32m    229\u001b[0m                     \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_worker_manager_loop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                     args=(self.worker_result_queue, self.data_queue, self.done_event, self.pin_memory,\n\u001b[0m\u001b[1;32m    231\u001b[0m                           maybe_device_id))\n\u001b[1;32m    232\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker_manager_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdaemon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n    _set_worker_signal_handlers()\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 42, in <listcomp>\n    _set_worker_signal_handlers()\n  File \"/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\", line 101, in __getitem__\n    sample = self.loader(path)\n  File \"/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\", line 147, in default_loader\n    return pil_loader(path)\n  File \"/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\", line 129, in pil_loader\n    img = Image.open(f)\n  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 2321, in open\n  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 370, in preinit\n    def preinit():\n  File \"/usr/local/lib/python3.6/dist-packages/PIL/PpmImagePlugin.py\", line 158, in <module>\n    Image.register_extensions(PpmImageFile.format, [\".pbm\", \".pgm\", \".ppm\"])\nAttributeError: module 'PIL.Image' has no attribute 'register_extensions'\n"
     ]
    }
   ],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z7wz3YWqU6nX"
   },
   "source": [
    "#Finetuning the convnet\n",
    "Load a pretrained Resnet 18 model and reset final fully connected layer.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "wkyoRYRx178U",
    "outputId": "deae861e-205d-4f5a-ae18-b0d05bb5d289"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.torch/models/resnet18-5c106cde.pth\n",
      "100%|██████████| 46827520/46827520 [00:00<00:00, 89191938.60it/s]\n"
     ]
    }
   ],
   "source": [
    "model_ft = models.resnet18(pretrained=True) # loading a pre-trained(trained on image net) resnet18 model from torchvision models\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 102) # changing the last layer for this dataset by setting last layer neurons to 102 as this dataset has 102 categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bxAc6D9QVO46"
   },
   "source": [
    "You can load a checkpoint from your my drive or any other place if you have saved it. you have to load weights of model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "ASm79Wag98cc",
    "outputId": "f771564a-d234-43d4-cdd9-5a542d8cad32"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-c98b83d4537b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/root/.torch/models/resnet18-5c106cde.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optim'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model'"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('/root/.torch/models/resnet18-5c106cde.pth')\n",
    "model_ft.load_state_dict(checkpoint['model'])\n",
    "optimizer_ft.load_state_dict(checkpoint['optim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kuO7i7qGscUA"
   },
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available() # if gpu is available then use it\n",
    "if use_gpu:\n",
    "    model_ft = model_ft.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # defining loss function\n",
    "\n",
    "\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.0001, momentum=0.9) # defining optimizer with learning rate set to 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FfWbTPqJUrip"
   },
   "source": [
    "#Training the model\n",
    "Now, let's write a general function to train a model.\n",
    "I also have written code to save the best checkpoint within Google drive for using next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7dWLDiE12BMs"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs=10):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                #scheduler.step()\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for data in dataloaders[phase]:\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "\n",
    "                # wrap them in Variable\n",
    "                if use_gpu:\n",
    "                    inputs = Variable(inputs.cuda())\n",
    "                    labels = Variable(labels.cuda())\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.data[0]\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = model.state_dict()\n",
    "                # saving a checkpoint to use for next time to save time used in training from scratch\n",
    "                state = {'model':model.state_dict(),'optim':optimizer.state_dict()}\n",
    "                torch.save(state,'drive/My Drive/flowers classification/point_resnet_best.pth')\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gPiIPHnrUzuH"
   },
   "source": [
    "#Visualizing the model predictions\n",
    "Generic function to display predictions for a few images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DF2SADrkstvy"
   },
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=8):\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "    \n",
    "\n",
    "    for i, data in enumerate(dataloaders['valid']):\n",
    "        inputs, labels = data\n",
    "        #print(labels)\n",
    "        if use_gpu:\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "        #print(labels)\n",
    "        #_, lab = torch.max(labels.data, 1)\n",
    "        outputs = model(inputs)\n",
    "        #print(outputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        #print(preds)\n",
    "\n",
    "        for j in range(inputs.size()[0]):\n",
    "            images_so_far += 1\n",
    "            ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "            ax.axis('off')\n",
    "            ax.set_title('class: {} predicted: {}'.format(class_names[labels.data[j]], class_names[preds[j]]))\n",
    "            imshow(inputs.cpu().data[j])\n",
    "\n",
    "            if images_so_far == num_images:\n",
    "                return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lac6x2STVs71"
   },
   "source": [
    "#Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 845
    },
    "colab_type": "code",
    "id": "ATaHZ6sttE0Q",
    "outputId": "e7a2e6ba-a07c-4a3c-982f-909a5de13c9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-5cf456209969>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_ft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-a631e678c725>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                 \u001b[0;31m# get the inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mbase_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    228\u001b[0m                 self.worker_manager_thread = threading.Thread(\n\u001b[1;32m    229\u001b[0m                     \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_worker_manager_loop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                     args=(self.worker_result_queue, self.data_queue, self.done_event, self.pin_memory,\n\u001b[0m\u001b[1;32m    231\u001b[0m                           maybe_device_id))\n\u001b[1;32m    232\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker_manager_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdaemon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n    _set_worker_signal_handlers()\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 42, in <listcomp>\n    _set_worker_signal_handlers()\n  File \"/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\", line 101, in __getitem__\n    sample = self.loader(path)\n  File \"/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\", line 147, in default_loader\n    return pil_loader(path)\n  File \"/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\", line 129, in pil_loader\n    img = Image.open(f)\n  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 2321, in open\n  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 370, in preinit\n    def preinit():\n  File \"/usr/local/lib/python3.6/dist-packages/PIL/PpmImagePlugin.py\", line 158, in <module>\n    Image.register_extensions(PpmImageFile.format, [\".pbm\", \".pgm\", \".ppm\"])\nAttributeError: module 'PIL.Image' has no attribute 'register_extensions'\n"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft,num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qkkzUKD4WXdG"
   },
   "source": [
    "# Checking Model's Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 827
    },
    "colab_type": "code",
    "id": "jIXRTXoS9urR",
    "outputId": "8f0e22fc-76ee-482e-abf9-a984b2451184"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-72e004aa6e49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvisualize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-43-444fcffea595>\u001b[0m in \u001b[0;36mvisualize_model\u001b[0;34m(model, num_images)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m#print(labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mbase_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    228\u001b[0m                 self.worker_manager_thread = threading.Thread(\n\u001b[1;32m    229\u001b[0m                     \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_worker_manager_loop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                     args=(self.worker_result_queue, self.data_queue, self.done_event, self.pin_memory,\n\u001b[0m\u001b[1;32m    231\u001b[0m                           maybe_device_id))\n\u001b[1;32m    232\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker_manager_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdaemon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n    _set_worker_signal_handlers()\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 42, in <listcomp>\n    _set_worker_signal_handlers()\n  File \"/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\", line 101, in __getitem__\n    sample = self.loader(path)\n  File \"/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\", line 147, in default_loader\n    return pil_loader(path)\n  File \"/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\", line 129, in pil_loader\n    img = Image.open(f)\n  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 2321, in open\n  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 370, in preinit\n    def preinit():\n  File \"/usr/local/lib/python3.6/dist-packages/PIL/PpmImagePlugin.py\", line 158, in <module>\n    Image.register_extensions(PpmImageFile.format, [\".pbm\", \".pgm\", \".ppm\"])\nAttributeError: module 'PIL.Image' has no attribute 'register_extensions'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa6433d2a58>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_model(model_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MlPl451nWgDv"
   },
   "source": [
    "#Checking model's accuracy on test set(This is top-1 accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GfcVPAlz91_b"
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(model, data):\n",
    "    model.eval()\n",
    "    if use_gpu:\n",
    "      model.cuda()    \n",
    "    running_corrects = 0\n",
    "    test_acc = 0\n",
    "    \n",
    "    #with torch.no_grad():\n",
    "    for idx, (inputs, labels) in enumerate(dataloaders[data]):\n",
    "        if use_gpu:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        # obtain the outputs from the model\n",
    "        outputs = model.forward(Variable(inputs))\n",
    "        # max provides the (maximum probability, max value)\n",
    "        #_, predicted = outputs.max(dim=1)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        running_corrects += torch.sum(predicted == labels)\n",
    "    test_acc = running_corrects / dataset_sizes['test']\n",
    "    print('Test Accuracy: {:.4f}'.format(test_acc))\n",
    "    '''# check the \n",
    "    if idx == 0:\n",
    "        print(predicted) #the predicted class\n",
    "        print(torch.exp(_)) # the predicted probability\n",
    "    equals = predicted == labels.data\n",
    "    if idx == 0:\n",
    "        print(equals)\n",
    "    print(equals.float().mean())'''\n",
    "calc_accuracy(model_ft, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SkBCRqGTWr0G"
   },
   "source": [
    "#Preparation for finding top-5 accuracy\n",
    "Here i have defined a use full class and a function to find top-1 and top-5 accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "leuU5KSTMd2x"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1mmUQpp1NHPR"
   },
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    #with torch.no_grad():\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zopjzHzRMVhT"
   },
   "outputs": [],
   "source": [
    "\n",
    "def calc_accuracy(model, data):\n",
    "    model.eval()\n",
    "    if use_gpu:\n",
    "      model.cuda()    \n",
    "    running_corrects = 0\n",
    "    test_acc = 0\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    \n",
    "    for idx, (inputs, labels) in enumerate(dataloaders[data]):\n",
    "        if use_gpu:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        # obtain the outputs from the model\n",
    "        outputs = model.forward(Variable(inputs))\n",
    "        prec1, prec5 = accuracy(outputs, Variable(labels), topk=(1, 5))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "        top5.update(prec5[0], inputs.size(0))\n",
    "        \n",
    "    return top1 ,top5\n",
    "    \n",
    "top1 ,top5 = calc_accuracy(model_ft, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_tzmV5wBXHzC"
   },
   "source": [
    "#Top-1 Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VdnU5u_wPsnI"
   },
   "outputs": [],
   "source": [
    "top1.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ObolyXNzXSgm"
   },
   "source": [
    "#Top-5 Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fTq6IwVhQQwa"
   },
   "outputs": [],
   "source": [
    "top5.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mWrbv4xpXmk8"
   },
   "source": [
    "#Inference for classification\n",
    "Now I have written a function to use a trained network for inference. That is, I'll pass an image into the network and predict the class of the flower in the image. I have written a function called predict that takes an image and a model, then returns the top  K  most likely classes along with the probabilities. It should look like\n",
    "\n",
    "probs, classes = predict(image_path, model)\n",
    "print(probs)\n",
    "print(classes)\n",
    "> [ 0.01558163  0.01541934  0.01452626  0.01443549  0.01407339]\n",
    "> ['70', '3', '45', '62', '55']\n",
    "First I have to handle processing the input image such that it can be used in my network.\n",
    "\n",
    "#Image Preprocessing\n",
    "I want to use PIL to load the image . It's best to write a function that preprocesses the image so it can be used as input for the model. This function should process the images in the same manner used for training.\n",
    "\n",
    "First, I resized the images where the shortest side is 256 pixels, keeping the aspect ratio. This has  done with the thumbnail or resize methods. Then I have cropped out the center 224x224 portion of the image.\n",
    "\n",
    "Color channels of images are typically encoded as integers 0-255, but the model expected floats 0-1. I have converted the values. It's easiest with a Numpy array, which I had got from a PIL image like so np_image = np.array(pil_image).\n",
    "\n",
    "As before, the network expects the images to be normalized in a specific way. For the means, it's [0.485, 0.456, 0.406] and for the standard deviations [0.229, 0.224, 0.225]. I had subtracted the means from each color channel, then divided by the standard deviation.\n",
    "\n",
    "And finally, PyTorch expects the color channel to be the first dimension but it's the third dimension in the PIL image and Numpy array. I have reordered dimensions using ndarray.transpose. The color channel needs to be first and retain the order of the other two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kdk-gKidB_ar"
   },
   "outputs": [],
   "source": [
    "def process_image(image_path):\n",
    "    ''' \n",
    "    Scales, crops, and normalizes a PIL image for a PyTorch       \n",
    "    model, returns an Numpy array\n",
    "    '''\n",
    "    # Open the image\n",
    "    from PIL import Image\n",
    "    img = Image.open(image_path)\n",
    "    # Resize\n",
    "    if img.size[0] > img.size[1]:\n",
    "        img.thumbnail((10000, 256))\n",
    "    else:\n",
    "        img.thumbnail((256, 10000))\n",
    "    # Crop \n",
    "    left_margin = (img.width-224)/2\n",
    "    bottom_margin = (img.height-224)/2\n",
    "    right_margin = left_margin + 224\n",
    "    top_margin = bottom_margin + 224\n",
    "    img = img.crop((left_margin, bottom_margin, right_margin,   \n",
    "                      top_margin))\n",
    "    # Normalize\n",
    "    img = np.array(img)/255\n",
    "    mean = np.array([0.485, 0.456, 0.406]) #provided mean\n",
    "    std = np.array([0.229, 0.224, 0.225]) #provided std\n",
    "    img = (img - mean)/std\n",
    "    \n",
    "    # Move color channels to first dimension as expected by PyTorch\n",
    "    img = img.transpose((2, 0, 1))\n",
    "    \n",
    "    return img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uiIQ9gWGCQMG"
   },
   "outputs": [],
   "source": [
    "def imshow(image, ax=None, title=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    # PyTorch tensors assume the color channel is first\n",
    "    # but matplotlib assumes is the third dimension\n",
    "    image = image.transpose((1, 2, 0))\n",
    "    \n",
    "    # Undo preprocessing\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    \n",
    "    # Image needs to be clipped between 0 and 1\n",
    "    image = np.clip(image, 0, 1)\n",
    "    \n",
    "    ax.imshow(image)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wkCjv_D0CRiH"
   },
   "outputs": [],
   "source": [
    "image_path = 'flowers/test/1/image_06752.jpg'\n",
    "img = process_image(image_path)\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b6QKymHwY43b"
   },
   "source": [
    "#Class Prediction\n",
    "OnceI have got images in the correct format, I have written a function for making predictions with my model. A common practice is to predict the top 5 or so (usually called top- K ) most probable classes. I have calculated the class probabilities then find the  K  largest values.\n",
    "\n",
    "To get the top  K  largest values in a tensor I have used x.topk(k). This method returns both the highest k probabilities and the indices of those probabilities corresponding to the classes. I have converted these indices to image names using class_names got from cat_to_name.json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P8w8z3uLCTSF"
   },
   "outputs": [],
   "source": [
    "def predict(image_path, model, top_num=5):\n",
    "    # Process image\n",
    "    img = process_image(image_path)\n",
    "    \n",
    "    # Numpy -> Tensor\n",
    "    image_tensor = torch.from_numpy(img).type(torch.FloatTensor)\n",
    "    # Add batch of size 1 to image\n",
    "    model_input = image_tensor.unsqueeze(0)\n",
    "    \n",
    "    # Probs\n",
    "    probs = torch.exp(model.forward(Variable(model_input.cuda())))\n",
    "    \n",
    "    # Top probs\n",
    "    top_probs, top_labs = probs.topk(top_num)\n",
    "    top_probs, top_labs =top_probs.data, top_labs.data\n",
    "    top_probs = top_probs.cpu().numpy().tolist()[0] \n",
    "    top_labs = top_labs.cpu().numpy().tolist()[0]\n",
    "    #print(top_labs)\n",
    "    # Convert indices to classes\n",
    "    '''idx_to_class = {val: key for key, val in    \n",
    "                                      model.class_to_idx.items()}\n",
    "    top_labels = [idx_to_class[lab] for lab in top_labs]\n",
    "    top_flowers = [cat_to_name[idx_to_class[lab]] for lab in top_labs]'''\n",
    "    top_flowers = [class_names[lab] for lab in top_labs]\n",
    "    return top_probs, top_flowers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1jTDhViuZ03D"
   },
   "source": [
    "#Sanity Checking\n",
    "Now I have used a trained model for predictions. Even if the testing accuracy is high, it's always good to check that there aren't obvious bugs. I have used matplotlib to plot the probabilities for the top 5 classes as a bar graph, along with the input image. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vT9YGJurCfLW"
   },
   "outputs": [],
   "source": [
    "def plot_solution(image_path, model):\n",
    "    # Set up plot\n",
    "    plt.figure(figsize = (6,10))\n",
    "    ax = plt.subplot(2,1,1)\n",
    "    # Set up title\n",
    "    flower_num = image_path.split('/')[2]\n",
    "    title_ = cat_to_name[flower_num]\n",
    "    # Plot flower\n",
    "    img = process_image(image_path)\n",
    "    imshow(img, ax, title = title_);\n",
    "    # Make prediction\n",
    "    probs, flowers = predict(image_path, model) \n",
    "    # Plot bar chart\n",
    "    plt.subplot(2,1,2)\n",
    "    sns.barplot(x=probs, y=flowers, color=sns.color_palette()[0]);\n",
    "    plt.show()\n",
    "image_path = 'flowers/test/90/image_04432.jpg'\n",
    "plot_solution(image_path, model_ft)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "102 Flowers classification.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
